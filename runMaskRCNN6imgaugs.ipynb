{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqZQkm5dueL0",
        "outputId": "980e560f-8b3c-4913-a7ef-6922e5988121"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'wgisd' already exists and is not an empty directory.\n",
            "Drive already mounted at /googledrive; to attempt to forcibly remount, call drive.mount(\"/googledrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#@title\n",
        "! git clone https://github.com/thsant/wgisd.git\n",
        "HEIGHT, WIDTH = 1365, 2048\n",
        "MASKTHRESH = 0.5\n",
        "from google.colab import drive\n",
        "drive.mount('/googledrive')\n",
        "from random import shuffle\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.ssd import det_utils\n",
        "from torchvision.ops import nms\n",
        "import torchvision\n",
        "import logging\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torch\n",
        "import warnings\n",
        "import random\n",
        "import colorsys\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import patches\n",
        "from matplotlib.patches import Polygon\n",
        "from skimage.color import label2rgb\n",
        "import torch.nn.functional as F\n",
        "from torchvision.ops import roi_align\n",
        "from torchvision.models.detection.roi_heads import project_masks_on_boxes\n",
        "from time import time\n",
        "from pathlib import Path\n",
        "import json\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "class WGISDMaskedDataset(Dataset):\n",
        "    def __init__(self, root, transforms=None, source='train'):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        \n",
        "        if source not in ('train', 'test', 'valid'):\n",
        "            print('source should be by \"train\" or \"test\"')\n",
        "            return None\n",
        "\n",
        "        srcpref = source\n",
        "        if source == 'valid':\n",
        "            srcpref = 'test'\n",
        "        source_path = os.path.join(root, f'{srcpref}_masked.txt')\n",
        "        with open(source_path, 'r') as fp:\n",
        "          lines = fp.readlines()\n",
        "          ids = [l.rstrip() for l in lines]# removes /n at the end of each line\n",
        "\n",
        "        self.imgs = [os.path.join(root, 'data', f'{id}.jpg') for id in ids]\n",
        "        self.masks = [os.path.join(root, 'data', f'{id}.npz') for id in ids]\n",
        "        self.boxes = [os.path.join(root, 'data', f'{id}.txt') for id in ids]\n",
        "\n",
        "        #performing additional dataset split test -> test, valid\n",
        "        if source == 'test':\n",
        "            self.imgs = self.imgs[len(self.imgs) // 2 : ]\n",
        "            self.masks = self.masks[len(self.masks) // 2 : ]\n",
        "            self.boxes = self.boxes[len(self.boxes) // 2 : ]\n",
        "        elif source == 'valid':\n",
        "            self.imgs = self.imgs[ : len(self.imgs) // 2 ]\n",
        "            self.masks = self.masks[ : len(self.masks) // 2 ]\n",
        "            self.boxes = self.boxes[ : len(self.boxes) // 2 ]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.imgs[idx]\n",
        "        mask_path = self.masks[idx]\n",
        "        box_path = self.boxes[idx]\n",
        "\n",
        "        img = cv2.imread(img_path)\n",
        "        cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        if self.transforms is None:\n",
        "            pass\n",
        "        else:\n",
        "            augmented = self.transforms(image=img) \n",
        "            img = augmented['image']\n",
        "\n",
        "        img = np.array(img)\n",
        "        # Normalize\n",
        "        img = (img - img.min()) / np.max([img.max() - img.min(), 1])\n",
        "        img = np.moveaxis(img, -1, 0)\n",
        "        img = torch.as_tensor(img, dtype=torch.float32)  \n",
        "\n",
        "\n",
        "        wgisd_masks = np.load(mask_path)['arr_0'].astype(np.uint8)\n",
        "        masks = np.moveaxis(wgisd_masks, -1, 0) \n",
        "\n",
        "        num_objs = masks.shape[0]\n",
        "        all_text = np.loadtxt(box_path, delimiter = \" \", dtype = np.float32)\n",
        "        wgisd_boxes = all_text[:,1:]\n",
        "        assert(wgisd_boxes.shape[0] == num_objs)\n",
        "\n",
        "        labels = np.ones(num_objs, dtype=np.int64)\n",
        "\n",
        "        # According to WGISD:\n",
        "        #\n",
        "        # These text files follows the \"YOLO format\"\n",
        "        # \n",
        "        # CLASS CX CY W H\n",
        "        # \n",
        "        # class is an integer defining the object class â€“ the dataset presents \n",
        "        # only the grape class that is numbered 0, so every line starts with \n",
        "        # this \"class zero\" indicator. The center of the bounding box is the \n",
        "        # point (c_x, c_y), represented as float values because this format \n",
        "        # normalizes the coordinates by the image dimensions. To get the \n",
        "        # absolute position, use (2048 c_x, 1365 c_y). The bounding box \n",
        "        # dimensions are given by W and H, also normalized by the image size.\n",
        "        #\n",
        "        # Torchvision's Mask R-CNN expects absolute coordinates.\n",
        "        _, height, width = img.shape\n",
        "\n",
        "        boxes = []\n",
        "        for box in wgisd_boxes:\n",
        "            x1 = box[0] - box[2]/2\n",
        "            x2 = box[0] + box[2]/2\n",
        "            y1 = box[1] - box[3]/2\n",
        "            y2 = box[1] + box[3]/2\n",
        "            boxes.append([x1 * width, y1 * height, x2 * width, y2 * height])\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "        image_id = torch.tensor([idx])\n",
        "\n",
        "        target = {\n",
        "            \"boxes\": boxes,\n",
        "            \"labels\": labels,\n",
        "            \"masks\": masks,\n",
        "            \"image_id\": image_id\n",
        "        }\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "def filter_results(results : dict, confidence_thresh = 0.8):\n",
        "    res = results.copy()\n",
        "    scores = results['scores']\n",
        "    res['boxes'] = results['boxes'][scores > confidence_thresh]\n",
        "    res['labels'] = results['labels'][scores >confidence_thresh]\n",
        "    res['masks'] = results['masks'][scores>confidence_thresh]\n",
        "    res['scores'] = results['scores'][scores > confidence_thresh]\n",
        "    return res\n",
        "\n",
        "def nmscombined(orig_predicted, iou_thresh = 0.3):\n",
        "    '''\n",
        "    performs non max suppression on the results\n",
        "    '''\n",
        "    keep = nms(orig_predicted['boxes'], orig_predicted['scores'], iou_thresh)\n",
        "    final_pred = orig_predicted\n",
        "    final_pred['boxes'] = final_pred['boxes'][keep]\n",
        "    final_pred['masks'] = final_pred['masks'][keep]\n",
        "    final_pred['scores'] = final_pred['scores'][keep]\n",
        "    final_pred['labels'] = final_pred['labels'][keep]\n",
        "    return final_pred\n",
        "\n",
        "def print_datsetslens():\n",
        "    train=WGISDMaskedDataset('./wgisd/', source='train')\n",
        "    test=WGISDMaskedDataset('./wgisd/', source='test')\n",
        "    valid = WGISDMaskedDataset('./wgisd/', source='valid')\n",
        "    print('trainlen:', len(train))\n",
        "    print('testlen:', len(test))\n",
        "    print('validlen:', len(valid))\n",
        "    den = len(train) + len(valid) + len(test)\n",
        "    print(f'split proportion: {len(train) / den} : {len(test) / den} : {len(valid) / den}')\n",
        "\n",
        "class DiceLoss(torch.nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(DiceLoss, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, smooth=1):\n",
        "        \n",
        "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
        "        inputs = torch.sigmoid(inputs)       \n",
        "        \n",
        "        #flatten label and prediction tensors\n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "        \n",
        "        intersection = (inputs * targets).sum()                            \n",
        "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
        "        \n",
        "        return 1 - dice\n",
        "\n",
        "class IoULoss(torch.nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(IoULoss, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, smooth=1):\n",
        "        \n",
        "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
        "        inputs = torch.sigmoid(inputs)       \n",
        "        \n",
        "        #flatten label and prediction tensors\n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "        \n",
        "        #intersection is equivalent to True Positive count\n",
        "        #union is the mutually inclusive area of all labels & predictions \n",
        "        intersection = (inputs * targets).sum()\n",
        "        total = (inputs + targets).sum()\n",
        "        union = total - intersection \n",
        "        \n",
        "        IoU = (intersection + smooth)/(union + smooth)\n",
        "                \n",
        "        return 1 - IoU\n",
        "\n",
        "\n",
        "class LossIntoMaskRCNN(object):\n",
        "    def __init__(self, lossfunction = 'bincrossentr_wl'):\n",
        "        if lossfunction == 'bincrossentr_wl':\n",
        "            self.loss = F.binary_cross_entropy_with_logits\n",
        "        elif lossfunction == 'mse':\n",
        "            self.loss = F.mse_loss\n",
        "        elif lossfunction =='dice':\n",
        "            self.loss = DiceLoss()\n",
        "        elif lossfunction == 'iou':\n",
        "            self.loss = IoULoss()\n",
        "    def lossintomaskrcnn(self, mask_logits, proposals, gt_masks, gt_labels, mask_matched_idxs, loss = 'bincrossentr_wl'): \n",
        "        # type: (Tensor, List[Tensor], List[Tensor], List[Tensor], List[Tensor]) \n",
        "        \"\"\" \n",
        "        Arguments: \n",
        "            proposals (list[BoxList]) \n",
        "            mask_logits (Tensor) \n",
        "            targets (list[BoxList]) \n",
        "\n",
        "        Return: \n",
        "            mask_loss (Tensor): scalar tensor containing the loss \n",
        "        \"\"\" \n",
        "\n",
        "        discretization_size = mask_logits.shape[-1] \n",
        "        labels = [l[idxs] for l, idxs in zip(gt_labels, mask_matched_idxs)] \n",
        "        mask_targets = [ \n",
        "            project_masks_on_boxes(m, p, i, discretization_size) \n",
        "            for m, p, i in zip(gt_masks, proposals, mask_matched_idxs) \n",
        "        ] \n",
        "\n",
        "        labels = torch.cat(labels, dim=0) \n",
        "        mask_targets = torch.cat(mask_targets, dim=0) \n",
        "\n",
        "        # torch.mean (in binary_cross_entropy_with_logits) doesn't \n",
        "        # accept empty tensors, so handle it separately \n",
        "        if mask_targets.numel() == 0: \n",
        "            return mask_logits.sum() * 0 \n",
        "\n",
        "        mask_loss = self.loss( \n",
        "            mask_logits[torch.arange(labels.shape[0], device=labels.device), labels], mask_targets \n",
        "        ) \n",
        "        return mask_loss\n",
        "\n",
        "def availablelosses():\n",
        "    return ['bincrossentr_wl', 'dice', 'iou', 'mse']\n",
        "def availablelosses_tofunctions():\n",
        "    return {'bincrossentr_wl':F.binary_cross_entropy_with_logits, 'dice':DiceLoss(), 'iou':IoULoss(),'mse':F.mse_loss}\n",
        "\n",
        "def setloss(loss = 'bincrossentr_wl'):\n",
        "    lossIMRCNN = LossIntoMaskRCNN(loss)\n",
        "    torchvision.models.detection.roi_heads.maskrcnn_loss = lossIMRCNN.lossintomaskrcnn \n",
        "\n",
        "def eager_outputs_foo(losses, detections):\n",
        "    return losses, detections\n",
        "\n",
        "def get_maskrcnn(pretrained = True):\n",
        "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=pretrained)\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    #replace head predictor (boxes)\n",
        "    NUM_CLASSES=2\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES)\n",
        "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "    hidden_layer = 256\n",
        "    #replace mask predictor\n",
        "    fg_iou_thresh = 0.5 #min IoU to be considered as positive\n",
        "    bg_iou_thresh = 0.5 #max IoU to be considered as negative\n",
        "    model.roi_heads.proposal_matcher.low_threshold = bg_iou_thresh\n",
        "    model.roi_heads.proposal_matcher.high_threshold = fg_iou_thresh\n",
        "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, NUM_CLASSES)\n",
        "    # model.eager_outputs = eager_outputs_foo\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install -U albumentations\n",
        "# ! pip uninstall opencv-python-headless\n",
        "# ! pip install opencv-python-headless==4.1.2.30"
      ],
      "metadata": {
        "id": "R5w-xhqpv0vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import albumentations as A\n",
        "traintransforms = A.Compose([\n",
        "    A.OneOf([\n",
        "        A.MotionBlur( p =1),\n",
        "        A.GaussianBlur(p=1, blur_limit=(3,7))\n",
        "    ], p =0.35),\n",
        "    A.OneOf([\n",
        "        A.GaussNoise(p=1, var_limit = (70,140)),\n",
        "        A.ISONoise(p=1, intensity = (0.3, 0.7))\n",
        "    ], p=0.35),\n",
        "    A.OneOf([\n",
        "        A.RandomBrightnessContrast(p=1)\n",
        "    ], p = 0.7)\n",
        "])\n",
        "testtransforms = None\n",
        "# for i in range(12):\n",
        "#     train[0]"
      ],
      "metadata": {
        "id": "jR71325aUFpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODELPATH = '/googledrive/MyDrive/'"
      ],
      "metadata": {
        "id": "lh3-MlyukdlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Datasets lens:"
      ],
      "metadata": {
        "id": "_KMsQ_1JpM_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_datsetslens()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "es_yUpBhpPEH",
        "outputId": "7b340ed6-020c-4068-d3e3-f47cdc7c2685"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainlen: 110\n",
            "testlen: 14\n",
            "validlen: 14\n",
            "split proportion: 0.7971014492753623 : 0.10144927536231885 : 0.10144927536231885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model training procedure:"
      ],
      "metadata": {
        "id": "SwaTdulqvdsL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPYtK8x00lLW"
      },
      "outputs": [],
      "source": [
        "def trainmaskrcnn(lossfunction = 'dice'): # took 23 minutes\n",
        "    setloss(lossfunction)\n",
        "    MODELNAME='model6'+lossfunction + '.pt'\n",
        "    LOGSUFFIX = '_LOGS'\n",
        "    savemodel = True\n",
        "    warnings.filterwarnings('ignore')\n",
        "    torch.manual_seed(0)\n",
        "    LEARNING_RATE = 0.003\n",
        "    WEIGHT_DECAY = 0.0005\n",
        "    MOMENTUM = 0.9\n",
        "    NUM_EPOCHS = 14\n",
        "    BATCH_SIZE = 2\n",
        "    filepath = os.path.join(MODELPATH, MODELNAME.split('.')[0] + LOGSUFFIX + '.log')\n",
        "    myfile = Path(filepath)\n",
        "    myfile.touch(exist_ok=True)\n",
        "    logging.basicConfig(\n",
        "        format='%(levelname)s: %(asctime)s %(message)s',\n",
        "        level = logging.DEBUG,\n",
        "        filename = filepath,\n",
        "        filemode='a'\n",
        "    )\n",
        "    logging.info(\"START\")\n",
        "    logging.info(f\"lossfunction: {lossfunction}\")\n",
        "\n",
        "    train = WGISDMaskedDataset('./wgisd', source = 'train', transforms = traintransforms)\n",
        "    trainloader = DataLoader(train, BATCH_SIZE, shuffle=True, num_workers = 2, collate_fn=lambda s: tuple(zip(*s)))\n",
        "    valid = WGISDMaskedDataset('./wgisd', source = 'valid', transforms = testtransforms)\n",
        "    validloader = DataLoader(valid, BATCH_SIZE, shuffle=True, num_workers = 2, collate_fn=lambda s: tuple(zip(*s)))\n",
        "\n",
        "    model = get_maskrcnn()\n",
        "\n",
        "    DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    logging.info(f'DEVICE: {DEVICE}')\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = True\n",
        "    model.train()\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = torch.optim.SGD(params, lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=6, gamma=0.33, verbose = False)\n",
        "    trainlosses = []\n",
        "    trainmasklosses = []\n",
        "    validlosses = []\n",
        "    validmasklosses = []\n",
        "\n",
        "    for epoch in range(1, NUM_EPOCHS + 1):\n",
        "        logging.info(f'Current epoch: {epoch} of {NUM_EPOCHS}')\n",
        "        lossacc= 0.0\n",
        "        lossaccmask = 0.0\n",
        "        model.train()\n",
        "        #TODO add cross validation\n",
        "        for batchid, (images, targets) in enumerate(trainloader, 1):\n",
        "            images = [image.to(DEVICE) for image in images]\n",
        "            targets = [ {k : v.to(DEVICE) for k, v in t.items()}for t in targets]\n",
        "            loss_dict = model(images, targets)\n",
        "            loss = sum(loss_dict.values())\n",
        "            lossmask = loss_dict['loss_mask']\n",
        "\n",
        "            #backprop\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            #logs\n",
        "            lossacc += loss.item()\n",
        "            lossaccmask += lossmask.item()\n",
        "\n",
        "        logging.info(f'Finalepoch{epoch} loss on train: lossacc:{lossacc}')\n",
        "        trainlosses.append(lossacc)\n",
        "        trainmasklosses.append(lossaccmask)\n",
        "\n",
        "        model.train()\n",
        "        logging.info(f'Evaluation...')\n",
        "        validaccloss = 0.0\n",
        "        validacclossmask = 0.0\n",
        "        with torch.no_grad():\n",
        "            for i, (imgs, trgts) in enumerate(validloader, 1):\n",
        "                imgs = [img.to(DEVICE) for img in imgs]\n",
        "                trgts = [ {k : v.to(DEVICE) for k, v in t.items()}for t in trgts]\n",
        "                loss = model(imgs, trgts)\n",
        "                validaccloss += sum(loss.values()).item()\n",
        "                lossmask = loss['loss_mask']\n",
        "                validacclossmask += lossmask.item()\n",
        "        logging.info(f'valid losses: {validaccloss}')\n",
        "        validlosses.append(validaccloss)\n",
        "        validmasklosses.append(validacclossmask)\n",
        "        lr_scheduler.step()\n",
        "    model.eval()\n",
        "    if savemodel == True:\n",
        "        torch.save(model.state_dict(), os.path.join(MODELPATH, MODELNAME))\n",
        "    logging.info('FINISHED')\n",
        "    data=(trainlosses, trainmasklosses), (validlosses, validmasklosses)\n",
        "    with open( MODELPATH + MODELNAME + \"_LOSSES.json\" , \"w\" ) as write:\n",
        "        json.dump( data , write )\n",
        "    return (trainlosses, trainmasklosses), (validlosses, validmasklosses)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(MODELPATH)\n",
        "availablelosses()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B96Xe37IO69o",
        "outputId": "f6b48566-cafc-4325-be15-d87b9485f282"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/googledrive/MyDrive/\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bincrossentr_wl', 'dice', 'iou', 'mse']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 20min\n",
        "(trainlosses, trainmasklosses), (validlosses, validmasklosses) = trainmaskrcnn(lossfunction = 'bincrossentr_wl')"
      ],
      "metadata": {
        "id": "2nr308FIyh85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "359331dfb35742889d9e866125bbf261",
            "6ead5b50d37c408cbb42c65500ab9ad5",
            "a6e3df97725c4a2387bedc958c21d0d3",
            "22181e308228431588dc3204915476cd",
            "8bae0825e9e04894990d37c4ec965ed0",
            "1c2615cfadbe4edd9f8d39fad8e4173e",
            "727db6056e714c1085cc347ff059bc6c",
            "c627646892fd4c9abb54ea7fdca9c79a",
            "4dc8deaf0e82442b9d4ea6dfcab8b449",
            "2ba22dce80c34d1c9154d1906a2df76d",
            "ba874e30156d49098321e09e1a3fced1"
          ]
        },
        "outputId": "bd04f4e3-9cbd-4d35-a96a-c5478757a90d"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /root/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "359331dfb35742889d9e866125bbf261",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/170M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2NHfoC6xpdE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07a2d5a0-1072-401f-e4dc-0d1a6f5909fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "imaugs\t\t\t\tmodel6bincrossentr_wl.pt\n",
            "MaskRCNNimgaugs6.ipynb\t\tmodel6bincrossentr_wl.pt_LOSSES.json\n",
            "model5bincrossentr_wl_LOGS.log\trunMaskRCNNdifferentlossfoos5.ipynb\n",
            "model6bincrossentr_wl_LOGS.log\tUntitled0.ipynb\n"
          ]
        }
      ],
      "source": [
        "!ls /googledrive/MyDrive/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "MaskRCNNimgaugs6.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "359331dfb35742889d9e866125bbf261": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6ead5b50d37c408cbb42c65500ab9ad5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a6e3df97725c4a2387bedc958c21d0d3",
              "IPY_MODEL_22181e308228431588dc3204915476cd",
              "IPY_MODEL_8bae0825e9e04894990d37c4ec965ed0"
            ]
          }
        },
        "6ead5b50d37c408cbb42c65500ab9ad5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a6e3df97725c4a2387bedc958c21d0d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1c2615cfadbe4edd9f8d39fad8e4173e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_727db6056e714c1085cc347ff059bc6c"
          }
        },
        "22181e308228431588dc3204915476cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c627646892fd4c9abb54ea7fdca9c79a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 178090079,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 178090079,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4dc8deaf0e82442b9d4ea6dfcab8b449"
          }
        },
        "8bae0825e9e04894990d37c4ec965ed0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2ba22dce80c34d1c9154d1906a2df76d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170M/170M [00:02&lt;00:00, 78.4MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ba874e30156d49098321e09e1a3fced1"
          }
        },
        "1c2615cfadbe4edd9f8d39fad8e4173e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "727db6056e714c1085cc347ff059bc6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c627646892fd4c9abb54ea7fdca9c79a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4dc8deaf0e82442b9d4ea6dfcab8b449": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2ba22dce80c34d1c9154d1906a2df76d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ba874e30156d49098321e09e1a3fced1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}